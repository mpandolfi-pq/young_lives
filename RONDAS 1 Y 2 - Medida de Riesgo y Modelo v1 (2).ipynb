{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615a5326",
   "metadata": {},
   "source": [
    "# Rondas 1 y 2\n",
    "\n",
    "\n",
    "\n",
    "### 1. Medida de Riesgo\n",
    "* definir variables target\n",
    "* preprocesar variables \n",
    "* calcular indice de riesgo\n",
    "* definir etiquetas\n",
    "\n",
    "### 2. Modelo\n",
    "* gridsearch\n",
    "* entrenar regresion\n",
    "* ROC\n",
    "* Matriz de Confusion\n",
    "* Reporte de clasificacion\n",
    "* Otros analisis (variables y quintiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ed59fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing \n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, recall_score, roc_curve, auc, accuracy_score,f1_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold, KFold\n",
    "import statsmodels.api as sm\n",
    "import time\n",
    "\n",
    "#%pylab inline\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "#formato de visualizaciones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98eb9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk_calculation(pond, df_with_dummies, ronda, var_socioec, var_salud, var_antrp):\n",
    "    if pond == 'standar':\n",
    "        var_target = var_socioec + var_salud + var_antrp\n",
    "\n",
    "        pond_socioec  = 1/len(var_socioec)\n",
    "        pond_salud = 1/len(var_salud)\n",
    "        pond_antrp = 1/len(var_antrp)\n",
    "\n",
    "        data_ronda = df_with_dummies[df_with_dummies.ronda == ronda].sort_values(by = 'childid')\n",
    "\n",
    "        min_max_scaler = MinMaxScaler() \n",
    "        data_ronda.loc[:,var_target] = min_max_scaler.fit_transform(data_ronda.loc[:,var_target])\n",
    "\n",
    "        score_salud = (data_ronda.loc[:,var_salud] * pond_salud).sum(axis=1)\n",
    "        score_antrp = (data_ronda.loc[:,var_antrp] * pond_antrp).sum(axis=1)\n",
    "        score_socioec = (data_ronda.loc[:,var_socioec] * pond_socioec).sum(axis=1)\n",
    "        \n",
    "        score_final = (1/3) * score_salud + (1/3) * score_antrp + (1/3) * score_socioec \n",
    "\n",
    "        data_ronda[\"risk\"] = score_final\n",
    "        data_ronda[\"risk\"] = np.where(data_ronda.deceased == 1, 1, data_ronda.risk)\n",
    "    else:\n",
    "        print(\"need to adjust manually\")\n",
    "    \n",
    "    return data_ronda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d100e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def label_observation(y_prob, prop, classes = None):\n",
    "    if classes == 'binary':\n",
    "        cut = round((1-prop)/0.1)\n",
    "        y_pred = pd.DataFrame(y_prob, columns = ['y_test_prob'])\n",
    "        y_pred = pd.qcut(y_pred.y_test_prob, 10, labels = False, duplicates = 'drop')\n",
    "        y_pred = np.where(y_pred >= cut,1,0)\n",
    "    else:\n",
    "        print('modify function')\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad65cf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_curve_plot(y_test, y_prob, data_set = None):\n",
    "    fpr_log,tpr_log,thr_log = roc_curve(y_test, y_prob)\n",
    "    df = pd.DataFrame(dict(fpr=fpr_log, tpr=tpr_log, thr = thr_log))\n",
    "    plt.axis([0, 1.01, 0, 1.01])\n",
    "    plt.xlabel('1 - Specificty')\n",
    "    plt.ylabel('TPR / Sensitivity')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.plot(df['fpr'],df['tpr'])\n",
    "    plt.plot(np.arange(0,1, step =0.01), np.arange(0,1, step =0.01))\n",
    "    plt.show() \n",
    "    \n",
    "    if data_set == 'train':\n",
    "        return print('AUC train =', auc(fpr_log, tpr_log))\n",
    "    elif data_set == 'test':\n",
    "        return print('AUC test =', auc(fpr_log, tpr_log))\n",
    "    else:\n",
    "        return print('AUC =', auc(fpr_log, tpr_log))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7822f6fb",
   "metadata": {},
   "source": [
    "## Carga de dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06cc67b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>childid</th>\n",
       "      <th>yc</th>\n",
       "      <th>ronda</th>\n",
       "      <th>inround</th>\n",
       "      <th>panel12345</th>\n",
       "      <th>deceased</th>\n",
       "      <th>dint</th>\n",
       "      <th>placeid</th>\n",
       "      <th>clustid</th>\n",
       "      <th>typesite</th>\n",
       "      <th>...</th>\n",
       "      <th>maths_raw</th>\n",
       "      <th>ppvt_raw</th>\n",
       "      <th>rawscre</th>\n",
       "      <th>reading_raw</th>\n",
       "      <th>sppvt_raw</th>\n",
       "      <th>srawscre</th>\n",
       "      <th>score_cog</th>\n",
       "      <th>rscorelang_cog</th>\n",
       "      <th>rscorelang_ppvt</th>\n",
       "      <th>score_ppvt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9/2/2002</td>\n",
       "      <td>PE01C01</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3/26/2007</td>\n",
       "      <td>PE01C01</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.000</td>\n",
       "      <td>305.212</td>\n",
       "      <td>280.317</td>\n",
       "      <td>22.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/10/2009</td>\n",
       "      <td>PE01C01</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>21.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/8/2013</td>\n",
       "      <td>PE01C01</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/11/2016</td>\n",
       "      <td>PE01C01</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    childid  yc  ronda  inround  panel12345  deceased       dint  placeid  \\\n",
       "0  PE011001   1      1        1           1       NaN   9/2/2002  PE01C01   \n",
       "1  PE011001   1      2        1           1       NaN  3/26/2007  PE01C01   \n",
       "2  PE011001   1      3        1           1       NaN  8/10/2009  PE01C01   \n",
       "3  PE011001   1      4        1           1       NaN   8/8/2013  PE01C01   \n",
       "4  PE011001   1      5        1           1       NaN  8/11/2016  PE01C01   \n",
       "\n",
       "   clustid  typesite  ...  maths_raw  ppvt_raw  rawscre  reading_raw  \\\n",
       "0    1.000     1.000  ...        NaN       NaN      NaN          NaN   \n",
       "1    1.000     1.000  ...        NaN       NaN   22.000          NaN   \n",
       "2    1.000     1.000  ...     21.000    80.000   80.000          NaN   \n",
       "3    1.000     1.000  ...        NaN       NaN      NaN          NaN   \n",
       "4    1.000     1.000  ...        NaN       NaN      NaN          NaN   \n",
       "\n",
       "   sppvt_raw  srawscre  score_cog  rscorelang_cog  rscorelang_ppvt  score_ppvt  \n",
       "0        NaN       NaN        NaN             NaN              NaN         NaN  \n",
       "1        NaN       NaN      9.000         305.212          280.317      22.000  \n",
       "2        NaN       NaN        NaN             NaN              NaN         NaN  \n",
       "3        NaN       NaN        NaN             NaN              NaN         NaN  \n",
       "4        NaN       NaN        NaN             NaN              NaN         NaN  \n",
       "\n",
       "[5 rows x 227 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importo los datos\n",
    "df = pd.read_csv(\"Desktop/TESIS/df_final.csv\", na_values=' ')\n",
    "df.rename(columns={'round': 'ronda'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e92dc",
   "metadata": {},
   "source": [
    "## Modificaciones y transformaciones de variables antes de dividir por ronda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "280a8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# modifico las variables para el target para que a mayor valor, mayor riesgo\n",
    "# CATEGORICAS\n",
    "\n",
    "df.chsex = np.where(df.chsex == 2,0,1) \n",
    "\n",
    "df.headsex = np.where(df.headsex == 2,0,1) \n",
    "\n",
    "df.caresex = np.where(df.caresex == 2,0,1) \n",
    "\n",
    "df['chhrel_mod'] = np.where(df.chhrel == 3,1,0)  # marco solo el caso \"Worse\", no el de \"Same\" o \"Better\"\n",
    "\n",
    "\n",
    "\n",
    "df['bmi_cat'] = 0\n",
    "df.loc[df.bmi < 18.5 , \"bmi_cat\"] = 2\n",
    "df.loc[(df.bmi >= 18.5) & (df.bmi <22), \"bmi_cat\"] = 0\n",
    "df.loc[(df.bmi >= 22) & (df.bmi <24.9) , \"bmi_cat\"] = 1\n",
    "df.loc[(df.bmi >= 24.9) & (df.bmi <29.9) , \"bmi_cat\"] = 2\n",
    "df.loc[(df.bmi >=29.9) , \"bmi_cat\"] = 3\n",
    "\n",
    "#pd.cut(x=df['bmi'], bins=[0, 18.5, 22, 24.9, 29.9, 500 ],ordered=False,\n",
    "#                     labels=[2, 0, 1, 2, 3])\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# modifico las variables para el target para que a mayor valor, mayor riesgo\n",
    "# CONTINUAS\n",
    "\n",
    "df[\"wi\"] = ((df[\"wi\"] - 1) * -1)\n",
    "df[\"hq\"] = ((df[\"hq\"] - 1) * -1)\n",
    "df[\"cd\"] = ((df[\"cd\"] - 1) * -1)\n",
    "df[\"sv\"] = ((df[\"sv\"] - 1) * -1)\n",
    "\n",
    "df.loc[df[\"wi\"].isna(),'wi'] = 1\n",
    "df.loc[df[\"hq\"].isna(),'hq'] = 1\n",
    "df.loc[df[\"cd\"].isna(),'cd'] = 1\n",
    "df.loc[df[\"sv\"].isna(),'sv'] = 1\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Quito variables y hago arreglos finales en todo el dataset\n",
    "\n",
    "# Paso a dummy variables PREDICTORAS que son NO ordinales.\n",
    "dummie_list = [\"headrel\",\"carerel\",\"carehead\",\"entype\",\"chrephealth4\",'chdisscale',\"region\"]\n",
    "dummies = pd.concat([pd.get_dummies(df[col]) for col in dummie_list], axis=1, keys=dummie_list)\n",
    "df_with_dummies = pd.concat([df,dummies],axis = 1)\n",
    "\n",
    "# paso a int la unica variable que quedó en formato \"objeto\"\n",
    "df.hospital.replace(['Yes', 'No'],[0,1], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89e7c9",
   "metadata": {},
   "source": [
    "## Definicion de variables para cada ronda\n",
    "Se mantienen separadas por si en algun futuro alguna ronda es modificada independientemente de las demas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16b0dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>childid</th>\n",
       "      <th>dint</th>\n",
       "      <th>placeid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>9/2/2002</td>\n",
       "      <td>PE01C01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>3/26/2007</td>\n",
       "      <td>PE01C01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>8/10/2009</td>\n",
       "      <td>PE01C01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>8/8/2013</td>\n",
       "      <td>PE01C01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PE011001</td>\n",
       "      <td>8/11/2016</td>\n",
       "      <td>PE01C01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    childid       dint  placeid\n",
       "0  PE011001   9/2/2002  PE01C01\n",
       "1  PE011001  3/26/2007  PE01C01\n",
       "2  PE011001  8/10/2009  PE01C01\n",
       "3  PE011001   8/8/2013  PE01C01\n",
       "4  PE011001  8/11/2016  PE01C01"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Para cada eje (salud, socioeconomico, etc)\n",
    "# definimos las variables que determinan si el niño estuvo expuesto a una situacion de riesgo o no\n",
    "# la idea es hacerlo para cada ronda\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# RONDA 1\n",
    "\n",
    "var_r1_socioec_cat = []\n",
    "var_r1_socioec_cont  = ['wi',\"hq\",\"cd\",\"sv\"]\n",
    "var_r1_socioec = var_r1_socioec_cont + var_r1_socioec_cat\n",
    "\n",
    "var_r1_salud_cat = [\"chhrel\",\"chmightdie\",\"deceased\"]\n",
    "var_r1_salud_cont = []\n",
    "var_r1_salud = var_r1_salud_cat + var_r1_salud_cont\n",
    "\n",
    "var_r1_familia_cat = []\n",
    "var_r1_familia_cont = []\n",
    "var_r1_familia = var_r1_familia_cat + var_r1_familia_cont\n",
    "\n",
    "var_r1_antrp_cat = ['stunting',\"bmi_cat\"]\n",
    "var_r1_antrp_cont = []\n",
    "var_r1_antrp = var_r1_antrp_cat + var_r1_antrp_cont\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# RONDA 2\n",
    "\n",
    "var_r2_socioec_cat = []\n",
    "var_r2_socioec_cont  = ['wi',\"hq\",\"cd\",\"sv\"]\n",
    "var_r2_socioec = var_r2_socioec_cont + var_r2_socioec_cat\n",
    "\n",
    "var_r2_salud_cat = [\"chhrel\",\"chmightdie\",\"chillness\",\"deceased\",\"chhrel_mod\"]\n",
    "var_r2_salud_cont = []\n",
    "var_r2_salud = var_r2_salud_cat + var_r2_salud_cont\n",
    "\n",
    "var_r2_familia_cat = []\n",
    "var_r2_familia_cont = []\n",
    "var_r2_familia = var_r2_familia_cat + var_r2_familia_cont\n",
    "\n",
    "var_r2_antrp_cat = ['stunting',\"bmi_cat\"]\n",
    "var_r2_antrp_cont = []\n",
    "var_r2_antrp = var_r2_antrp_cat + var_r2_antrp_cont\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# RONDA 3\n",
    "\n",
    "var_r3_socioec_cat = []\n",
    "var_r3_socioec_cont  = ['wi',\"hq\",\"cd\",\"sv\"]\n",
    "var_r3_socioec = var_r3_socioec_cont + var_r3_socioec_cat\n",
    "var_r3_salud_cat = [\"chhrel\",\"chmightdie\",\"chillness\",\"deceased\",\"chhrel_mod\"]\n",
    "var_r3_salud_cont = []\n",
    "var_r3_salud = var_r3_salud_cat + var_r3_salud_cont\n",
    "var_r3_familia_cat = []\n",
    "var_r3_familia_cont = []\n",
    "var_r3_familia = var_r3_familia_cat + var_r3_familia_cont\n",
    "var_r3_antrp_cat = ['stunting',\"bmi_cat\"]\n",
    "var_r3_antrp_cont = []\n",
    "var_r3_antrp = var_r3_antrp_cat + var_r3_antrp_cont\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# RONDA 4\n",
    "\n",
    "var_r4_socioec_cat = []\n",
    "var_r4_socioec_cont  = ['wi',\"hq\",\"cd\",\"sv\"]\n",
    "var_r4_socioec = var_r4_socioec_cont + var_r4_socioec_cat\n",
    "var_r4_salud_cat = [\"chhrel\",\"chmightdie\",\"chillness\",\"deceased\",\"chhrel_mod\"]\n",
    "var_r4_salud_cont = []\n",
    "var_r4_salud = var_r4_salud_cat + var_r4_salud_cont\n",
    "var_r4_familia_cat = []\n",
    "var_r4_familia_cont = []\n",
    "var_r4_familia = var_r4_familia_cat + var_r4_familia_cont\n",
    "var_r4_antrp_cat = ['stunting',\"bmi_cat\"]\n",
    "var_r4_antrp_cont = []\n",
    "var_r4_antrp = var_r4_antrp_cat + var_r4_antrp_cont\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# RONDA 5\n",
    "\n",
    "var_r5_socioec_cat = []\n",
    "var_r5_socioec_cont  = ['wi',\"hq\",\"cd\",\"sv\"]\n",
    "var_r5_socioec = var_r5_socioec_cont + var_r5_socioec_cat\n",
    "var_r5_salud_cat = [\"chhrel\",\"chmightdie\",\"chillness\",\"deceased\",\"chhrel_mod\"]\n",
    "var_r5_salud_cont = []\n",
    "var_r5_salud = var_r5_salud_cat + var_r5_salud_cont\n",
    "var_r5_familia_cat = []\n",
    "var_r5_familia_cont = []\n",
    "var_r5_familia = var_r5_familia_cat + var_r5_familia_cont\n",
    "var_r5_antrp_cat = ['stunting',\"bmi_cat\"]\n",
    "var_r5_antrp_cont = []\n",
    "var_r5_antrp = var_r5_antrp_cat + var_r5_antrp_cont\n",
    "\n",
    "df.loc[:, df.dtypes == object].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659055d7",
   "metadata": {},
   "source": [
    "## Calculo el Riesgo por Round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df431d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NewUser/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:400: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/Users/NewUser/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:401: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/Users/NewUser/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:400: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/Users/NewUser/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:401: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/Users/NewUser/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:400: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/Users/NewUser/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:401: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n",
      "/Users/NewUser/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:400: RuntimeWarning: All-NaN slice encountered\n",
      "  data_min = np.nanmin(X, axis=0)\n",
      "/Users/NewUser/opt/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/_data.py:401: RuntimeWarning: All-NaN slice encountered\n",
      "  data_max = np.nanmax(X, axis=0)\n"
     ]
    }
   ],
   "source": [
    "r1 = risk_calculation('standar', df_with_dummies, 1, var_r1_socioec, var_r1_salud, var_r1_antrp)\n",
    "r2 = risk_calculation('standar', df_with_dummies, 2, var_r2_socioec, var_r2_salud, var_r2_antrp)\n",
    "r3 = risk_calculation('standar', df_with_dummies, 3, var_r3_socioec, var_r3_salud, var_r3_antrp)\n",
    "r4 = risk_calculation('standar', df_with_dummies, 4, var_r4_socioec, var_r4_salud, var_r4_antrp)\n",
    "r5 = risk_calculation('standar', df_with_dummies, 5, var_r5_socioec, var_r5_salud, var_r5_antrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a2454c",
   "metadata": {},
   "source": [
    "### Divido el riesgo en deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb968a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1[\"risk_cat_1\"] = pd.qcut(r1[\"risk\"] , 10,labels = False)\n",
    "r2[\"risk_cat_2\"] = pd.qcut(r2[\"risk\"] , 10,labels = False)\n",
    "r3[\"risk_cat_3\"] = pd.qcut(r3[\"risk\"] , 10,labels = False)\n",
    "r4[\"risk_cat_4\"] = pd.qcut(r4[\"risk\"] , 10,labels = False)\n",
    "r5[\"risk_cat_5\"] = pd.qcut(r5[\"risk\"] , 10,labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65038a",
   "metadata": {},
   "source": [
    "### Visualizo la distribucion del indice de riesgo por ronda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f24afb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#pltrcParamsupdate({fontsize: 15})\n",
    "\n",
    "#pltrcParamsupdate({fontsize: 15})\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.hist(r1.risk, bins=20, color='lightblue')\n",
    "plt.title(\"Ronda 1\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(r2.risk, bins=20, color='lightblue')\n",
    "plt.title(\"Ronda 2\")\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(r3.risk, bins=20, color='lightblue')\n",
    "plt.title(\"Ronda 3\")\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.hist(r4.risk, bins=20, color='lightblue')\n",
    "plt.title(\"Ronda 4\")\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(r5.risk, bins=20, color='lightblue')\n",
    "plt.title(\"Ronda 5\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f246cee",
   "metadata": {},
   "source": [
    "## Armo el dataset para el entrenamiento\n",
    "* saco columnas que repiten informacion del target\n",
    "* separo en X e Y\n",
    "* estandarizo variables\n",
    "* relleno los NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc881713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 19 6 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/NewUser/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_asarray.py:102: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Quito variables \"administrativas\" y variables que repiten información del target\n",
    "# Por ejemplo índice de masa corporal y el flag the unserweight.\n",
    "\n",
    "admin = [ 'clustid','childid',\"ronda\",\"yc\",\"panel12345\",\"dint\",\"agemon\",'careid','dadid','headid','momid']\n",
    "\n",
    "repeated_info = [\"underweight\" , \"thinness\", \"drwaterq\" , \"toiletq\" , \"elecq\" , \"cookingq\",\"bmi\",\n",
    "                \"zwfa\",\"zhfa\",\"zbfa\",\"zwfl\",\"fwfl\",\"fhfa\",\"fwfa\",\"fbfa\",\"momyrdied\",\"dadyrdied\",'hospital','chweight']\n",
    "\n",
    "dependet = [\"risk\",\"risk_cat_1\",\"risk_cat_2\",'risk_cat_3',\"risk_cat_4\",'risk_cat_5']\n",
    "other = ['placeid']\n",
    "\n",
    "\n",
    "print(len(admin),\n",
    "      len(repeated_info),\n",
    "      len(dependet),\n",
    "      len(other))\n",
    "\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "#\"levlread\", \"levlwrit\",\"engrade\" , \"enrol\" refuse to answer. REVISAR.!!!!!!!!!!\n",
    "\n",
    "cols_to_keep = [\"inround\",\n",
    "    'agegr1','aniany','anibeeh','anicowm','anicowt','anidonk','anidrau',\n",
    "    'anifish','anifshr','anigoat','aniguin','anillam','animilk','aniothr',\n",
    "    'anioxen','anipigs','anipoul','anirabb','anirumi','anishee','anishri',\n",
    "    'anisnai','anispec','bcg','beca_yl','birth','birth_age','bonograt','bwdoc',\n",
    "    'bwght','careage','carecantread','caredu','careladder','caresex','chalcohol',\n",
    "    'chdisability','chhealth','chhprob','childloc','chillness',\n",
    "    'chinjury',\"chsmoke\",'chsex','cladder','commsch','commwork','credit',\n",
    "    \"chrephealth3\",\"chrephealth2\",\"chrephealth1\",\"careldr4yrs\",\n",
    "    'dadcantread','dadedu','dadlive',\"dadage\",'delivery','dpt',\n",
    "    \"engrade\",\"enrol\",\n",
    "    'female05','female1317','female1860','female61','female612','foodsec',\n",
    "    'hcare','hchore','headage','headedudu','headsex','hhsize','hib',\n",
    "    'hplay','hschool','hsleep','hstudy','htask','hwork',\n",
    "    'insur_yl','juntos',\n",
    "    'lang_raw',\"literate\",\"levlread\",\"levlwrit\",\n",
    "    'male05','male1317','male1860','male61','male612',\"marrcohab\",'marrcohab_age','maths_raw','measles','minsa_yl',\n",
    "    'momage','momcantread','momedu','momlive','numante','ownhouse','ownlandhse',\n",
    "    'polio','ppvt_raw','preprim','projoven_yl',\n",
    "    'rawscre','reading_raw','rscorelang_cog','rscorelang_ppvt','score_cog',\n",
    "    'score_ppvt','shcrime1','shcrime2','shcrime3','shcrime4','shcrime5','shcrime6','shcrime8',\n",
    "    'shecon1','shecon10','shecon11','shecon12','shecon14','shecon2','shecon3','shecon4','shecon5',\n",
    "    'shecon6','shecon7','shecon8','shecon9','shenv1','shenv10','shenv11','shenv12','shenv2','shenv3',\n",
    "    'shenv4','shenv5','shenv6','shenv7','shenv8','shenv9','shfam1','shfam10','shfam12','shfam13',\n",
    "    'shfam14','shfam15','shfam16','shfam17','shfam18','shfam2','shfam3','shfam4','shfam5','shfam6',\n",
    "    'shfam7','shfam8','shfam9','shhouse1','shhouse2','shhouse3','shother','shregul1','shregul2',\n",
    "    'shregul4','shregul5','shregul6','sisgrat_yl','sppvt_raw','srawscre',\n",
    "    'tetanus','typesite',\"timesch\",\n",
    "    ( 'headrel',  1.0),( 'headrel',  2.0),( 'headrel',  3.0),( 'headrel',  4.0),( 'headrel',  5.0),\n",
    "    ( 'headrel',  6.0),( 'headrel',  7.0),( 'carerel',  1.0),( 'carerel',  2.0),( 'carerel',  3.0),\n",
    "    ( 'carerel',  4.0),( 'carerel',  5.0),( 'carerel',  6.0),( 'carerel',  7.0),('carehead',  1.0),\n",
    "    ('carehead',  2.0),('carehead',  3.0),(  'entype',  1.0),(  'entype',  2.0),(  'entype',  3.0),\n",
    "    (  'entype',  4.0),(  'region', 31.0),(  'region', 32.0),(  'region', 33.0),(  'region', 88.0),\n",
    "               ]\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# separo dataset de predictoras de la variable dependiente\n",
    "\n",
    "X_r1 = r1.loc[:,cols_to_keep]\n",
    "#x_train = x_train.loc[:,~(x_train.isnull().values.all(axis=0))] eliminar todas las columnas con null values\n",
    "y_r1 = r1.risk_cat_1\n",
    "\n",
    "\n",
    "X_r2 = r2.loc[:,cols_to_keep]\n",
    "y_r2 = r2.risk_cat_2\n",
    "\n",
    "X_r3 = r3.loc[:,cols_to_keep]\n",
    "y_r3 = r3.risk_cat_3\n",
    "\n",
    "X_r4 = r4.loc[:,cols_to_keep]\n",
    "y_r4 = r4.risk_cat_4\n",
    "\n",
    "X_r5 = r5.loc[:,cols_to_keep]\n",
    "y_r5 = r5.risk_cat_5\n",
    "\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Reemplazo los nulos por cero.  REVISAR.!!!!!!!!!!\n",
    "\n",
    "X_r1 = X_r1.fillna(0)\n",
    "X_r2 = X_r2.fillna(0)\n",
    "X_r3 = X_r3.fillna(0)\n",
    "X_r4 = X_r4.fillna(0)\n",
    "X_r5 = X_r5.fillna(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf4863fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols_to_keep 202 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('cols_to_keep',len(cols_to_keep),'\\n')\n",
    "#print('still_need_work_to_include',len(still_need_to_work_on),'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf610a2b",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09e4a6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import LeaveOneOut, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c19d091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_r1, y_r2, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0141d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Estandarizo dataset de entrenamiento y test por separado\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_test)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca588041",
   "metadata": {},
   "source": [
    "### Leave One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b248975e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut  = 0.7\n",
    "y_train2 = np.where((y_train >= cut),1,0)\n",
    "auc_score_test = []\n",
    "auc_score_train = []\n",
    "scores = []\n",
    "d = {}\n",
    "\n",
    "model_performance = {}\n",
    "\n",
    "model = LogisticRegression(solver = \"liblinear\", \n",
    "                             penalty = 'l1', \n",
    "                             max_iter = 300\n",
    "                          )\n",
    "    \n",
    "loo = LeaveOneOut()\n",
    "for train_index , test_index in loo.split(X_train_scaled):\n",
    "    X_sub_train, X_sub_test, y_sub_train, y_sub_test = X_train_scaled.loc[train_index],X_train_scaled.loc[test_index], y_train2[train_index], y_train2[test_index]\n",
    "    model.fit(X_sub_train,y_sub_train)\n",
    "    #print(\"y_sub_test\",y_sub_test)\n",
    "    y_sub_test_pred = model.predict_proba(X_sub_test)[:,1:] \n",
    "    y_sub_test_pred = np.where(y_sub_test_pred > 0.5,1,0)\n",
    "    #print(\"y_sub_test_pred\",y_sub_test_pred)\n",
    "    score = np.where(y_sub_test_pred == y_sub_test,1,0)\n",
    "    scores.append(score)\n",
    "mean_score = sum(scores)/len(scores)\n",
    "d[str(cut)] = mean_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c43517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver = \"liblinear\", \n",
    "                             penalty = 'l1', \n",
    "                             max_iter = 300\n",
    "                          )\n",
    "\n",
    "train_and_evaluate(X_train_scaled, y_train, model, cv, splits = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "721f813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_logit(X_train_scaled, y_train, X_test_scaled, y_test ,cut, reg,cv, splits = None):\n",
    "    \n",
    "    \n",
    "    d = {}\n",
    "    performance = [] \n",
    "    it = 0\n",
    "    \n",
    "    # for each lambda\n",
    "    for j in reg:\n",
    "        start = time.time()\n",
    "        #print('reg',j)\n",
    "        model = LogisticRegression(solver = \"liblinear\", \n",
    "                                 penalty = 'l1', \n",
    "                                 max_iter = 300,\n",
    "                                 C = j\n",
    "                                  )\n",
    "       # final['reg'] = j\n",
    "        # for each possible cut\n",
    "        for i in cut:\n",
    "            decile  = round((1-i)/0.1)\n",
    "            y_train2 = np.where((y_train >= decile),1,0)\n",
    "            y_test2 = np.where((y_test >= decile),1,0)\n",
    "            #print('cut',i)\n",
    "            \n",
    "            # if training with LOO method\n",
    "            if cv == 'loo':\n",
    "                scores = []\n",
    "                loo = LeaveOneOut()\n",
    "                for train_index , test_index in loo.split(X_train_scaled):\n",
    "                    X_sub_train, X_sub_test, y_sub_train, y_sub_test = X_train_scaled.loc[train_index],X_train_scaled.loc[test_index], y_train2[train_index], y_train2[test_index]\n",
    "                    model.fit(X_sub_train,y_sub_train)\n",
    "                    y_sub_test_proba = model.predict_proba(X_sub_test)[:,1:] \n",
    "                    y_sub_test_pred = np.where(y_sub_test_proba > 0.5,1,0)\n",
    "                    score = np.where(y_sub_test_pred == y_sub_test,1,0)\n",
    "                    scores.append(score)\n",
    "                mean_score = float(sum(scores)/len(scores))\n",
    "                #a[str(i)] = mean_score\n",
    "                \n",
    "                # evaluation\n",
    "                model.fit(X_train_scaled,y_train2)\n",
    "                test_predictions = model.predict_proba(X_test_scaled)[:,1:]\n",
    "                train_predictions = model.predict_proba(X_train_scaled)[:,1:]\n",
    "                \n",
    "                fpr_log,tpr_log,thr_log = roc_curve(y_test2, test_predictions)\n",
    "                test_auc = auc(fpr_log, tpr_log)\n",
    "                fpr_log,tpr_log,thr_log = roc_curve(y_train2, train_predictions)\n",
    "                train_auc = auc(fpr_log, tpr_log)\n",
    "            \n",
    "                #print(j,i,mean_score)\n",
    "                data = dict(regularization = j,\n",
    "                                         proportion_class = i, \n",
    "                                         mean_score = mean_score,\n",
    "                                         test_auc =test_auc,\n",
    "                                         train_auc=train_auc,\n",
    "                           #              index=[it]\n",
    "                           )\n",
    "                #print(data)\n",
    "                \n",
    "                performance.append(data)\n",
    "                #print(\"regularization:\", j, \"class balance:\", i, \"iteration:\", it)\n",
    "                it += 1\n",
    "   \n",
    "            # if training with Kfold method\n",
    "            elif cv == 'kf':\n",
    "                \n",
    "                auc_score_test = []\n",
    "                auc_score_train = []\n",
    "                d = {}\n",
    "                for train_index , test_index in kf.split(X_train_scaled):\n",
    "                    X_sub_train, X_sub_test, y_sub_train, y_sub_test = X_train_scaled.loc[train_index],X_train_scaled.loc[test_index], y_train2[train_index], y_train2[test_index]\n",
    "                    model.fit(X_sub_train,y_sub_train)\n",
    "                    y_sub_test_proba = model.predict_proba(X_sub_test)[:,1:]\n",
    "                    fpr_log, tpr_log, thr_log = roc_curve(y_sub_test, y_sub_test_proba)\n",
    "                    test_auc = auc(fpr_log, tpr_log)\n",
    "                    y_sub_train_proba = model.predict_proba(X_sub_train)[:,1:]\n",
    "                    fpr_log, tpr_log, thr_log = roc_curve(y_sub_train, y_sub_train_proba)\n",
    "                    train_auc = auc(fpr_log, tpr_log)\n",
    "                    auc_score_test.append(test_auc)\n",
    "                    auc_score_train.append(train_auc)\n",
    "                    print(\"regularization:\", j, \"class balance:\", i, \"iteration:\", it)\n",
    "                b['mean_train_auc'] = sum(auc_score_train)/len(auc_score_train)\n",
    "                b['std_train_auc'] = (sum([((x - b['mean_train_auc']) ** 2) for x in [1,2,3]]) / len([1,2,3]))**0.5\n",
    "                b['mean_test_auc'] = sum(auc_score_test)/len(auc_score_test)\n",
    "                b['std_test_auc'] = (sum([((x - b['mean_test_auc']) ** 2) for x in [1,2,3]]) / len([1,2,3]))**0.5\n",
    "                a[str(i)] = b\n",
    "            \n",
    "            \n",
    "            print(\"finish time\", time.time() - start)\n",
    "    return performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "08a05e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_and_evaluate_logit(X_train_scaled,\n",
    "                                   y_train, \n",
    "                                   X_test_scaled,\n",
    "                                   y_test,\n",
    "                                   [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "                                   [0.01,0.1,1,5],\n",
    "                                   'loo')\n",
    "\n",
    "print(results[0])\n",
    "\n",
    "results_df = pd.json_normalize(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be81b233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>regularization</th>\n",
       "      <th>proportion_class</th>\n",
       "      <th>mean_score</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>train_auc</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.865</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.866</td>\n",
       "      <td>[15]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.849</td>\n",
       "      <td>0.857</td>\n",
       "      <td>[16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.856</td>\n",
       "      <td>[13]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.873</td>\n",
       "      <td>[24]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.881</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>5.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.873</td>\n",
       "      <td>[33]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.886</td>\n",
       "      <td>[23]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.000</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.887</td>\n",
       "      <td>[32]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.866</td>\n",
       "      <td>[25]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.861</td>\n",
       "      <td>[22]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5.000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.867</td>\n",
       "      <td>[34]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5.000</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.861</td>\n",
       "      <td>[31]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.836</td>\n",
       "      <td>0.835</td>\n",
       "      <td>[9]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.862</td>\n",
       "      <td>[12]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.833</td>\n",
       "      <td>[6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.845</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.763</td>\n",
       "      <td>0.831</td>\n",
       "      <td>0.824</td>\n",
       "      <td>[4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.826</td>\n",
       "      <td>0.893</td>\n",
       "      <td>[26]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.854</td>\n",
       "      <td>[11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.783</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.868</td>\n",
       "      <td>[21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>5.000</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.781</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.868</td>\n",
       "      <td>[30]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.817</td>\n",
       "      <td>0.817</td>\n",
       "      <td>[7]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.828</td>\n",
       "      <td>[3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.853</td>\n",
       "      <td>[18]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.000</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.895</td>\n",
       "      <td>[35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.860</td>\n",
       "      <td>[20]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.845</td>\n",
       "      <td>[10]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>5.000</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.861</td>\n",
       "      <td>[29]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.000</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.856</td>\n",
       "      <td>[27]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.854</td>\n",
       "      <td>[19]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.780</td>\n",
       "      <td>[8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.818</td>\n",
       "      <td>[2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.812</td>\n",
       "      <td>[1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.000</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.808</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.855</td>\n",
       "      <td>[28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.715</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    regularization  proportion_class  mean_score  test_auc  train_auc index\n",
       "17           0.100             0.900       0.903     0.859      0.865  [17]\n",
       "15           0.100             0.700       0.778     0.853      0.866  [15]\n",
       "16           0.100             0.800       0.808     0.849      0.857  [16]\n",
       "13           0.100             0.500       0.781     0.845      0.856  [13]\n",
       "24           1.000             0.700       0.763     0.844      0.873  [24]\n",
       "14           0.100             0.600       0.782     0.843      0.881  [14]\n",
       "33           5.000             0.700       0.761     0.843      0.873  [33]\n",
       "23           1.000             0.600       0.779     0.841      0.886  [23]\n",
       "32           5.000             0.600       0.777     0.841      0.887  [32]\n",
       "25           1.000             0.800       0.797     0.840      0.866  [25]\n",
       "22           1.000             0.500       0.771     0.839      0.861  [22]\n",
       "34           5.000             0.800       0.793     0.838      0.867  [34]\n",
       "31           5.000             0.500       0.769     0.836      0.861  [31]\n",
       "9            0.100             0.100       0.903     0.836      0.835   [9]\n",
       "12           0.100             0.400       0.793     0.833      0.862  [12]\n",
       "6            0.010             0.700       0.753     0.832      0.833   [6]\n",
       "5            0.010             0.600       0.759     0.831      0.845   [5]\n",
       "4            0.010             0.500       0.763     0.831      0.824   [4]\n",
       "26           1.000             0.900       0.897     0.826      0.893  [26]\n",
       "11           0.100             0.300       0.786     0.823      0.854  [11]\n",
       "21           1.000             0.400       0.783     0.823      0.868  [21]\n",
       "30           5.000             0.400       0.781     0.820      0.868  [30]\n",
       "7            0.010             0.800       0.793     0.817      0.817   [7]\n",
       "3            0.010             0.400       0.771     0.815      0.828   [3]\n",
       "18           1.000             0.100       0.902     0.813      0.853  [18]\n",
       "35           5.000             0.900       0.896     0.811      0.895  [35]\n",
       "20           1.000             0.300       0.786     0.810      0.860  [20]\n",
       "10           0.100             0.200       0.808     0.809      0.845  [10]\n",
       "29           5.000             0.300       0.784     0.804      0.861  [29]\n",
       "27           5.000             0.100       0.897     0.800      0.856  [27]\n",
       "19           1.000             0.200       0.811     0.797      0.854  [19]\n",
       "8            0.010             0.900       0.902     0.794      0.780   [8]\n",
       "2            0.010             0.300       0.772     0.793      0.818   [2]\n",
       "1            0.010             0.200       0.807     0.792      0.812   [1]\n",
       "28           5.000             0.200       0.808     0.792      0.855  [28]\n",
       "0            0.010             0.100       0.903     0.718      0.715   [0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.sort_values(by=['test_auc'],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c35f67",
   "metadata": {},
   "source": [
    "## KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe1858",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance = {}\n",
    "\n",
    "model = LogisticRegression(solver = \"liblinear\", \n",
    "                             penalty = 'l1', \n",
    "                             max_iter = 300\n",
    "                          )\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "it = 0\n",
    "\n",
    "for i in [0.1, 0.2, 0.3, 0.5]:\n",
    "    cut  = round((1-i)/0.1)\n",
    "    y_train2 = np.where((y_train >= cut),1,0)\n",
    "    it += 1\n",
    "    auc_score_test = []\n",
    "    auc_score_train = []\n",
    "    d = {}\n",
    "    for train_index , test_index in kf.split(X_train_scaled):\n",
    "        X_sub_train, X_sub_test, y_sub_train, y_sub_test = X_train_scaled.loc[train_index],X_train_scaled.loc[test_index], y_train2[train_index], y_train2[test_index]\n",
    "        model.fit(X_sub_train,y_sub_train)\n",
    "        y_sub_test_pred = model.predict_proba(X_sub_test)[:,1:]\n",
    "        fpr_log, tpr_log, thr_log = roc_curve(y_sub_test, y_sub_test_pred)\n",
    "        test_auc = auc(fpr_log, tpr_log)\n",
    "        \n",
    "        y_sub_train_pred = model.predict_proba(X_sub_train)[:,1:]\n",
    "        fpr_log, tpr_log, thr_log = roc_curve(y_sub_train, y_sub_train_pred)\n",
    "        train_auc = auc(fpr_log, tpr_log)\n",
    "        \n",
    "        \n",
    "        auc_score_test.append(test_auc)\n",
    "        auc_score_train.append(train_auc)\n",
    "        \n",
    "    d['mean_train_auc'] = sum(auc_score_train)/len(auc_score_train)\n",
    "    d['std_train_auc'] = (sum([((x - d['mean_train_auc']) ** 2) for x in [1,2,3]]) / len([1,2,3]))**0.5\n",
    "    d['mean_test_auc'] = sum(auc_score_test)/len(auc_score_test)\n",
    "    d['std_test_auc'] = (sum([((x - d['mean_test_auc']) ** 2) for x in [1,2,3]]) / len([1,2,3]))**0.5\n",
    "    \n",
    "    model_performance[str(cut)] = d\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12d9daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "logit = LogisticRegressionCV(solver = \"liblinear\", \n",
    "                             penalty = 'l1', \n",
    "                             max_iter = 300, \n",
    "                             cv = 3#LeaveOneOut()\n",
    "                          )\n",
    "groups = 10\n",
    "\n",
    "results = train_and_evaluate(prop, logit, X_train_scaled, y_train,X_test_scaled, y_test )\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99176037",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo final - Ronda 1 vs Ronda 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1655370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.where((y_train >= 7),1,0)\n",
    "y_test = np.where((y_test >= 7),1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5d9eda",
   "metadata": {},
   "source": [
    "### Fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be07ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''this cell is ment to help choosing regularization parameter lambda for l1 penalty'''\n",
    "'''\n",
    "grid={\"C\":[10, 1, .1, .01, 0.001], \n",
    "      \"penalty\":[\"l1\",\"l2\"], \n",
    "      \"solver\":[\"liblinear\"],  \n",
    "      \"max_iter\":[100,500]}# l1 lasso l2 ridge\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg_cv = GridSearchCV(logreg, grid, cv = LeaveOneOut(), return_train_score = True)\n",
    "logreg_cv.fit(X_r1_scaled, y_r2)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(\"accuracy :\",logreg_cv.best_score_)\n",
    "\n",
    "results = pd.DataFrame(logreg_cv.cv_results_)\n",
    "results[\"overfit\"] = results.mean_train_score - results.mean_test_score \n",
    "\n",
    "print(results.loc[results.rank_test_score == 1,\"overfit\"])\n",
    "#results.sort_values(by = 'rank_test_score')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67363bb2",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f631fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Final Model Training\n",
    "\n",
    "\n",
    "\n",
    "logit_cv = LogisticRegressionCV(solver = \"liblinear\", \n",
    "                               penalty = 'l1', \n",
    "                               cv = LeaveOneOut(), \n",
    "                               Cs = [0.01], \n",
    "                               fit_intercept=True)\n",
    "\n",
    "\n",
    "logit_cv.fit(X_train_scaled, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c23d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predigo probabilidades\n",
    "y_test_prob = logit_cv.predict_proba(X_test_scaled)[:,1:]\n",
    "y_train_prob = logit_cv.predict_proba(X_train_scaled)[:,1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf78559d",
   "metadata": {},
   "source": [
    "### Metricas de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945dbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_plot(y_train, y_train_prob, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e509e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curve_plot(y_test, y_test_prob, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a4b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Comparo contra un clasificador Dummy\n",
    "\n",
    "dummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train_scaled, y_train)\n",
    "y_dummy_predictions = dummy_majority.predict(X_test_scaled)\n",
    "dummy_majority.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basandome en la curva roc elijo punto de corte\n",
    "y_test_pred = label_observation(y_test_prob, 0.3, 'binary')\n",
    "confusion_matrix = confusion_matrix(np.array(y_test), y_test_pred)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98233974",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy: {:.2f}'.format(accuracy_score(y_test, y_test_pred)))\n",
    "print('Precision: {:.2f}'.format(precision_score(y_test, y_test_pred)))\n",
    "print('Recall: {:.2f}'.format(recall_score(y_test, y_test_pred)))\n",
    "print('F1: {:.2f}'.format(f1_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a517d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_test_pred, target_names=['0', '1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b35ece",
   "metadata": {},
   "source": [
    "#### Interpretación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e130f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_dict = {}\n",
    "for coef, feat in zip(logit_cv.coef_[0,:],X_r1.columns):\n",
    "    coef_dict[feat] = coef\n",
    "    \n",
    "coef_dict\n",
    "dict(sorted(coef_dict.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fe1c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "risk_per_round = pd.merge(r1.loc[:,[\"childid\",\"risk_cat_1\"]], \n",
    "                          r2.loc[:,[\"childid\",\"risk_cat_2\"]],\n",
    "                          how=\"inner\",\n",
    "                          on = [\"childid\"])\n",
    "\n",
    "risk_per_round = pd.merge(risk_per_round, \n",
    "                          r3.loc[:,[\"childid\",\"risk_cat_3\"]],\n",
    "                          how=\"inner\",\n",
    "                          on = [\"childid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c474dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_per_round.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debba75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_per_round = pd.concat([risk_per_round.reset_index(drop=True),\n",
    "                      pd.DataFrame(y_r3_prob, columns = ['y_r3_prob']),\n",
    "                     pd.DataFrame(y_r3_pred, columns = ['y_r3_pred'])],\n",
    "                     axis = 1)\n",
    "risk_per_round.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ff4b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "baja_categoria_12 = risk_per_round[risk_per_round.risk_cat_1 < risk_per_round.risk_cat_2]\n",
    "sube_categoria_12 = risk_per_round[risk_per_round.risk_cat_1 > risk_per_round.risk_cat_2]\n",
    "print(sube_categoria_12.shape,baja_categoria_12.shape)\n",
    "\n",
    "baja_categoria_23 = risk_per_round[risk_per_round.risk_cat_2 < risk_per_round.risk_cat_3]\n",
    "sube_categoria_23 = risk_per_round[risk_per_round.risk_cat_2 > risk_per_round.risk_cat_3]\n",
    "print(sube_categoria_23.shape, baja_categoria_23.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2cd5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1[r1[\"childid\"].isin(sube_categoria_12[\"childid\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3be5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1[r1[\"childid\"].isin(baja_categoria_12[\"childid\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbe98ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_per_round[\"groups\"] = pd.qcut(risk_per_round[\"y_r3_prob\"] , 5,labels = False)\n",
    "risk_per_round.groupby([\"groups\"]).agg({\"y_r3_prob\":[\"count\",\"min\",\"mean\",\"max\"],\n",
    "                                        #\"risk_cat_1\" : [\"mean\"],\n",
    "                                        #\"risk_cat_2\" : [\"mean\"],\n",
    "                                        \"risk_cat_3\" : [\"mean\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47a4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = pd.merge(r2,\n",
    "                    risk_per_round.loc[:,[\"childid\",\"risk_cat_3\"]],\n",
    "                    how=\"inner\",\n",
    "                    on = [\"childid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ace7aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "feature_importance = df_preds.groupby(['risk_cat_3']).agg({\"momedu\":[\"mean\"],\n",
    "                                                           \"delivery\":[\"mean\"],\n",
    "                                                           \"agegr1\":[\"mean\"],\n",
    "                                                           \"bwght\":[\"mean\"],\n",
    "                                                           \"aniany\":[\"mean\"],\n",
    "                                                           \"typesite\":[\"mean\"],\n",
    "                                                           ('region', 31.0):[\"mean\"]})\n",
    "\n",
    "feature_importance.columns = ['momedu','delivery',\"agegr1\",\"bwght\",\"aniany\",\"typesite\",'region_31']\n",
    "\n",
    "\n",
    "feature_importance = feature_importance.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb547e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c86d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pltrcParamsupdate({fontsize: 15})\n",
    "\n",
    "#['momedu','carehead','female1860','measles',\"headrel\",\"dadage\"]\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.subplot(2, 3, 1)\n",
    "x = feature_importance.risk_cat_3\n",
    "y = feature_importance.momedu\n",
    "plt.bar(x, y)\n",
    "plt.title(\"momedu\")\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "x = feature_importance.risk_cat_3\n",
    "y = feature_importance.delivery\n",
    "plt.bar(x, y)\n",
    "plt.title(\"delivery\")\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "x = feature_importance.risk_cat_3\n",
    "y = feature_importance.agegr1\n",
    "plt.bar(x, y)\n",
    "plt.title(\"agegr1\")\n",
    "\n",
    "plt.subplot(2, 3, 4)\n",
    "x = feature_importance.risk_cat_3\n",
    "y = feature_importance.region_31\n",
    "plt.bar(x, y)\n",
    "plt.title(\"region_31\")\n",
    "\n",
    "plt.subplot(2, 3, 5)\n",
    "x = feature_importance.risk_cat_3\n",
    "y = feature_importance.bwght\n",
    "plt.bar(x, y)\n",
    "plt.title(\"bwght\")\n",
    "\n",
    "plt.subplot(2, 3, 6)\n",
    "x = feature_importance.risk_cat_3\n",
    "y = feature_importance.aniany\n",
    "plt.bar(x, y)\n",
    "plt.title(\"aniany\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e9ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''C = [500,25,10, 1, .1, .001]\n",
    "reg = ['l1','l2']\n",
    "CV = [5,10]\n",
    "\n",
    "for penalty in reg:\n",
    "    for cv in CV:\n",
    "        for c in C:\n",
    "            print(penalty, cv, c)\n",
    "            logit_l1 = LogisticRegressionCV(penalty=penalty, Cs=[c], solver='liblinear', cv=cv)\n",
    "            logit_l1.fit(X_r1_scaled, y_r2)\n",
    "            print('C:', c)\n",
    "            #print('Coefficient of each feature:', logit_l1.coef_)\n",
    "            print('Training accuracy:', logit_l1.score(X_r1_scaled, y_r2))\n",
    "            print('Test accuracy:', logit_l1.score(X_r2_scaled, y_r3))\n",
    "            print('')\n",
    "   \n",
    "# https://chrisalbon.com/code/machine_learning/logistic_regression/logistic_regression_with_l1_regularization/\n",
    "\n",
    "'''\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Gridsearch\n",
    "'''\n",
    "grid = {\"Cs\":[10, 1, .1, 0.001], \n",
    "      \"penalty\":[\"l1\",\"l2\"], \n",
    "      \"solver\":[\"liblinear\"],  \n",
    "      \"cv\":[3,5,10],\n",
    "      \"max_iter\":[10000]}\n",
    "\n",
    "logreg = LogisticRegressionCV()\n",
    "logreg_cv = GridSearchCV(logreg,grid,return_train_score = True)\n",
    "logreg_cv.fit(X_r1_scaled,y_r2)\n",
    "\n",
    "print(\"tuned hpyerparameters :(best parameters) \", logreg_cv.best_params_ )\n",
    "print(\"accuracy :\",logreg_cv.best_score_)\n",
    "\n",
    "results = pd.DataFrame(logreg_cv.cv_results_)\n",
    "results[\"overfit\"] = results.mean_train_score - results.mean_test_score '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
